{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<div style=\"font-size:83px; font-weight:bold; text-align: center;\">Learn You a Neural Net! </div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`whoami`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`stu`  \n",
    "Machine Learning Engineer @Opendoor  \n",
    "@mstewart141  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal: Puzzle through the mechanics of a fully connected neural net on our way to implementing one in a modular/reusable style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per Wikipedia:\n",
    "> An ANN is based on a collection of connected units or **neurons**... the output of each artificial neuron is calculated by a **non-linear function** of the **sum of its inputs**... Artificial neurons and edges typically have a **weight** that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection... Typically, neurons are organized in **layers**... Signals travel from the first (input), to the last (output) layer (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](ann2.jpeg)\n",
    "[(_source_)](https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial neurons and edges typically have a **weight** that adjusts as learning proceeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Ok but how do we make learning *proceed*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Almost always, we rely on a technique in the gradient descent family of algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Per Wikipedia:\n",
    "> Gradient descent is a first-order iterative optimization algorithm for finding the __minimum__ of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First, some data initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import enum\n",
    "from typing import List, Callable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import base, linear_model\n",
    "from sympy import var, plot, diff, solve, exp, tanh, functions, stats\n",
    "from toolz import pipe\n",
    "\n",
    "Tensor = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1],  # XOR problem\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [0, 1, 0],\n",
    "              [1, 0, 0],\n",
    "              [1, 1, 1]])\n",
    "Y = np.array([[0, 1, 1, 1, 1, 0]]).T\n",
    "\n",
    "sig = lambda z: 1 / (1 + np.exp(-z))\n",
    "sig_prime = lambda z: z * (1 - z)  # three lines of calculus later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### _XOR_  is a bit wonky.\n",
    "\n",
    "### Linear model? No dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_model.LogisticRegression(C=1e18).fit(X, Y.ravel()).predict(X).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What would a NN / MLP look like in a for loop?\n",
    "\n",
    "## _(a.k.a. gotta start somewhere)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Aside: we'll use `sigmoid` as a non-linear activation because it is widely known:\n",
    "\n",
    "# $$\\begin{equation} \\sigma(z) = 1\\space /\\space {(1+e^{-z})} \\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "betas0_1 = np.random.random(size=(3, 4)) - 0.5\n",
    "betas1_2 = np.random.random(size=(4, 1)) - 0.5\n",
    "\n",
    "lr = 16\n",
    "\n",
    "for i in range(30000):\n",
    "    layer0 = X\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "else:\n",
    "    print(np.round(layer2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is this so? Chain rule!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# $Cost = C(\\sigma(Z(X W)))$\n",
    "\n",
    "## $where \\space\\space C(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# $\\begin{split}C'(W) &= C'(\\sigma) \\cdot \\sigma'(Z) \\cdot Z'(W) \\\\\n",
    "      &= (\\hat{y} -y) \\cdot \\sigma'(Z) \\cdot X\\end{split}$\n",
    "      \n",
    "(_latex [source](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html)_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ok, your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now, what could go wrong with the above plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ok! But, how can we generalize this??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LayerKind(enum.Enum):\n",
    "    def __init__(self, fn: Callable, fn_prime: Callable):\n",
    "        self.fn = fn\n",
    "        self.fn_prime = fn_prime\n",
    "\n",
    "K = LayerKind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "'''Backward propagate gradient info for a mXk layer where prior layer was mXn and next layer is mXj.\n",
    "\n",
    "So, layer input is mXn, `this_layer_loss` is mXk, and betas are nXk. Grad best be nXk like our errors!\n",
    "\n",
    "self.activated: mXk\n",
    "this_layer_loss: mXk\n",
    "error_signal: mXk\n",
    "self.betas: nXk\n",
    "grad: nXk\n",
    "self.input: mXn\n",
    "prev_layer_loss: mXn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class NN(base.BaseEstimator):\n",
    "    pass\n",
    "#     def __init__(self):\n",
    "#     def _forward(self):\n",
    "#     def _backward(self):\n",
    "#     def fit(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# y_hat = NN(layers=[]).fit(X, Y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "all(y_hat == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ok, your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Can we implement a better stopping criterion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<div style=\"font-size:80px; font-weight:bold; text-align: center;\"> Questions? </div>\n",
    "<br>\n",
    "  \n",
    "  \n",
    "_@[mstewart141](https://twitter.com/mstewart141) [twitter, github, linkedin]_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
